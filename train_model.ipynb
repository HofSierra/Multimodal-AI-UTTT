{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0929ac21-cbe3-47be-8e91-e985b4b44408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: flash-attn 2.3.6\n",
      "Uninstalling flash-attn-2.3.6:\n",
      "  Successfully uninstalled flash-attn-2.3.6\n",
      "Found existing installation: xformers 0.0.33.post2\n",
      "Uninstalling xformers-0.0.33.post2:\n",
      "  Successfully uninstalled xformers-0.0.33.post2\n",
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Requirement already satisfied: unsloth in /opt/conda/lib/python3.10/site-packages (2025.11.6)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.57.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (4.3.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.48.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (11.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (25.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.11.6 in /opt/conda/lib/python3.10/site-packages (from unsloth) (2025.11.6)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: torch>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from unsloth) (2.9.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from unsloth) (0.24.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from unsloth) (2.2.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unsloth) (7.0.0)\n",
      "Requirement already satisfied: tyro in /opt/conda/lib/python3.10/site-packages (from unsloth) (0.9.35)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from unsloth) (6.31.1)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Using cached xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth) (3.5.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from unsloth) (0.2.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from unsloth) (0.18.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from unsloth) (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /opt/conda/lib/python3.10/site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /opt/conda/lib/python3.10/site-packages (from unsloth) (0.35.2)\n",
      "Requirement already satisfied: trl!=0.19.0,<=0.24.0,>=0.18.2 in /opt/conda/lib/python3.10/site-packages (from unsloth) (0.24.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (1.13.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: torchao>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.6->unsloth) (0.14.1)\n",
      "Requirement already satisfied: cut_cross_entropy in /opt/conda/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.6->unsloth) (25.1.1)\n",
      "Requirement already satisfied: msgspec in /opt/conda/lib/python3.10/site-packages (from unsloth_zoo>=2025.11.6->unsloth) (0.20.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: importlib_metadata in /opt/conda/lib/python3.10/site-packages (from diffusers->unsloth) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.10/site-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth) (14.2.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth) (1.8.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth) (4.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Using cached xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "Installing collected packages: xformers\n",
      "Successfully installed xformers-0.0.33.post2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y flash_attn xformers\n",
    "!pip install unsloth transformers accelerate datasets bitsandbytes pandas pillow packaging ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a2449e-620a-41fd-a376-e6cc9ca3f40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, Trainer, AutoProcessor\n",
    "from datasets import Dataset, load_from_disk\n",
    "from PIL import Image as PILImage\n",
    "from config import *\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6943f1f2-17c1-469c-b944-9bbf949cbc37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "tokenizer = None\n",
    "processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e03f2e6-36f0-4c63-a78a-b48ebfe89d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vlm_model():\n",
    "    global model, tokenizer, processor\n",
    "    print(f\"Loading Model: {MODEL_NAME}\")\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = MODEL_NAME,\n",
    "        max_seq_length = MAX_SEQ_LENGTH,\n",
    "        dtype = DTYPE,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token_id = processor.tokenizer.pad_token_id \n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    print(\"Model and Tokenizer loaded successfully\")\n",
    "    return model, tokenizer, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d855ff-e96e-4027-aa98-6c3dfaf57381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultimodalDataCollator(data):\n",
    "    \"\"\"\n",
    "    Data collator responsible for batching, tokenization, image processing, \n",
    "    and applying label masking for multimodal training.\n",
    "\n",
    "    1. Extracts images and conversation messages.\n",
    "    2. Converts the conversation messages (System + User + Assistant) into a \n",
    "       single text prompt string using the tokenizer's chat template.\n",
    "    3. Processes images and tokenizes the full text prompt into a batch using \n",
    "       the VLM's multimodal processor.\n",
    "    4. CRITICALLY: Applies label masking (setting loss to -100) to all tokens \n",
    "       belonging to the System and User turns, ensuring the model only learns \n",
    "       to predict the Assistant's (target JSON) response.\n",
    "    \"\"\"\n",
    "    global model, processor, tokenizer\n",
    "    if model is None or processor is None or tokenizer is None:\n",
    "        raise ValueError(\"Model, processor, tokenizer must be loaded globally before using this.\")\n",
    "\n",
    "    # 1. Extract Images and Messages\n",
    "    images = [item[\"image\"].convert(\"RGB\") for item in data]\n",
    "    messages = [item[\"messages\"] for item in data]\n",
    "\n",
    "    # 2. Generate the raw prompt strings for the FULL conversation\n",
    "    text_prompts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for conversation in messages\n",
    "    ]\n",
    "\n",
    "    # 3. Process Images and Tokenize Text (Full Batch)\n",
    "    try: \n",
    "        # The processor handles both image preprocessing (to image_pixel_values) \n",
    "        # and full text tokenization for the VLM, resulting in a padded batch.\n",
    "        inputs = processor(\n",
    "            images = images,\n",
    "            text = text_prompts,\n",
    "            return_tensors = \"pt\",\n",
    "            padding = True,\n",
    "            max_length = processor.tokenizer.model_max_length,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to create batch. Actual error: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 4. Manual Label Masking: Clone input_ids to create the initial labels tensor\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    # 5. Determine the masking cutoff point for each example\n",
    "    for i, conversation in enumerate(messages):\n",
    "        # We need the tokenized length of the [system, user] part to know where to stop masking\n",
    "        system_user_conversation = conversation[:2] \n",
    "        \n",
    "        # A. Generate the System + User string (text only)\n",
    "        # we must use the full multimodal processor on the System + User part. \n",
    "        # The previous attempt failed because the text-only tokenizer does not \n",
    "        # account for the image patch tokens inserted by the VLM processor.\n",
    "        \n",
    "        # 1. Generate the System + User string (includes <image> placeholder)\n",
    "        system_user_text = tokenizer.apply_chat_template(\n",
    "            system_user_conversation,\n",
    "            tokenize=False,\n",
    "            # This is critical to include the final <|assistant|> turn token\n",
    "            add_generation_prompt=True \n",
    "        )\n",
    "\n",
    "        # 2. Use the FULL multimodal processor on this single example to get the length.\n",
    "        # This correctly tokenizes the text and accounts for the image tokens.\n",
    "        try:\n",
    "            system_user_tokens = processor(\n",
    "                images = [images[i]], # Pass the single image from the batch\n",
    "                text = system_user_text,\n",
    "                return_tensors = \"pt\",\n",
    "                padding = False, # Do not pad single example\n",
    "            ).input_ids.squeeze()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing single prompt for masking length: {e}\")\n",
    "            system_user_tokens = torch.tensor([-1])\n",
    "\n",
    "\n",
    "        # Calculate the length for masking\n",
    "        if system_user_tokens.dim() == 0 or system_user_tokens[0] == -1:\n",
    "             # Handle error case by skipping (loss will be computed for full sequence, which is wrong)\n",
    "             print(f\"Warning: Masking calculation failed for sample {i}. Using safe, non-masked sequence.\")\n",
    "             cutoff_length = 0 # No mask\n",
    "        else:\n",
    "            # The number of tokens corresponding to the System + User part + <|assistant|> token\n",
    "            cutoff_length = system_user_tokens.shape[0]\n",
    "\n",
    "        # Mask out the System and User tokens in the padded batch by setting them to -100\n",
    "        # The loss function ignores tokens with label -100.\n",
    "        labels[i, :cutoff_length] = -100\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3709bb-f86f-4f1d-ace0-0828d8bcc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_training(examples):\n",
    "    messages_list = []\n",
    "    batch_size = len(examples.get(\"image\", []))\n",
    "\n",
    "    if batch_size == 0:\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        if not isinstance(examples.get(\"image\", [None]*batch_size)[i], PILImage.Image):\n",
    "            messages_list.append([])\n",
    "            continue\n",
    "\n",
    "        allowed_squares = examples[\"allowed_squares\"][i]\n",
    "        unplayable_boards = examples[\"unplayable_boards\"][i]\n",
    "        best_move_dict = examples[\"best_move\"][i]\n",
    "        player_turn = examples[\"player\"][i]\n",
    "        cot_text = examples[\"chain_of_thought\"][i]\n",
    "        legal_moves = examples[\"legal_moves\"][i]\n",
    "        ascii_board = examples[\"ascii_board\"][i]\n",
    "            \n",
    "        if isinstance(allowed_squares, (list, tuple)) and len(allowed_squares) == 2:\n",
    "            ACTIVE_GLOBAL_ROW, ACTIVE_GLOBAL_COL = allowed_squares\n",
    "            active_board_desc = f\"The active board coordinates are Global[{ACTIVE_GLOBAL_ROW}, {ACTIVE_GLOBAL_COL}] (highlighted in GREEN).\"\n",
    "        else:\n",
    "            active_board_desc = \"The player has a FREE MOVE, and can play in any board that is NOT UNPLAYABLE.\"\n",
    "\n",
    "\n",
    "        unplayable_list_str = json.dumps(unplayable_boards)    \n",
    "        best_move_json = json.dumps(best_move_dict)\n",
    "        legal_moves_str = json.dumps(legal_moves, indent=2)\n",
    "\n",
    "        # --- System Prompt (defines model's role) ---\n",
    "        system_content = (\n",
    "            \"You are an expert Ultimate Tic-Tac-Toe player. \"\n",
    "            \"Your task is to determine the optimal move for the current player. \"\n",
    "            \"Your output **MUST** be a single, raw JSON object containing the chosen move.\"\n",
    "        )\n",
    "\n",
    "        # --- User Prompt (provides all game context) ---\n",
    "        user_prompt = (\n",
    "            f\"Player: {player_turn} (X=Player 1, O=Player 2)\\n\"\n",
    "            f\"Analyze the board state in the image and determine the optimal move.\\n\\n\"\n",
    "            \n",
    "            f\"--- CRITICAL UTTT RULES ---\\n\"\n",
    "            f\"* **UNPLAYABLE BOARDS:** The following Global Boards are WON or TIED and **STRICTLY FORBIDDEN** for play:\\n\"\n",
    "            f\"    Unplayable Boards (Global R, C): {unplayable_list_str}\\n\"\n",
    "            f\"* **ACTIVE BOARD CONSTRAINT:** {active_board_desc}\\n\"\n",
    "            f\"    -   If the highlighted board is UNPLAYABLE, the player gets a **FREE MOVE**.\\n\"\n",
    "            f\"    -   Otherwise, the move **MUST** be in the highlighted board.\\n\"\n",
    "            \n",
    "            f\"\\n--- BOARD VISUALIZATION ---\\n\"\n",
    "            f\"Use the image and the following ASCII diagram for coordinate reference:\\n\"\n",
    "            f\"{ascii_board}\\n\\n\"\n",
    "            \n",
    "            f\"CRITICAL RULE: All coordinates (global_row, global_col, local_row, local_col) MUST be **0, 1 or 2**.\\n\"\n",
    "            f\"The set of all legal moves is provided for reference:\\n{legal_moves_str}\\n\"\n",
    "            \n",
    "            f\"\\n*** Output the optimal move as a single, raw JSON object. ***\"\n",
    "        )\n",
    "\n",
    "        assistant_content = \"\"\n",
    "        if cot_text:\n",
    "            assistant_content += f\"<think>{cot_text}</think>\\n\"\n",
    "            assistant_content += f\"{best_move_json}\\n\"\n",
    "        else:\n",
    "            assistant_content += f\"{best_move_json}\"\n",
    "            \n",
    "        messages_for_current_example = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_content}]},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": user_prompt},\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_content}]}\n",
    "        ]\n",
    "        messages_list.append(messages_for_current_example)\n",
    "\n",
    "    examples[\"messages\"] = messages_list\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6835286b-f793-48a9-9365-eb40a8178f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fine_tuning():\n",
    "    model, tokenizer, processor = load_vlm_model()\n",
    "\n",
    "    print(\"Applying PEFT (QLoRA) layer...\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = 32,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_alpha = 32,\n",
    "        lora_dropout = 0,\n",
    "        bias = \"none\",\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        random_state = 42,\n",
    "        embedding_layer_names = [\"vision_tower.image_projection\"]\n",
    "    )\n",
    "\n",
    "    print(\"Loading training dataset...\")\n",
    "    try: \n",
    "        raw_train_dataset = Dataset.from_parquet(DATASET_TRAIN_PATH)\n",
    "        print(f\"Dataset loaded with {len(raw_train_dataset)} examples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Loading evaluation dataset...\")\n",
    "    try: \n",
    "        raw_eval_dataset = Dataset.from_parquet(DATASET_EVALUATION_PATH)\n",
    "        print(f\"Dataset loaded with {len(raw_eval_dataset)} examples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Formatting datasets for multimodal training...\")\n",
    "\n",
    "    columns_needed_for_map = [\"image\", \"player\", \"allowed_squares\", \"best_move\",\n",
    "                              \"chain_of_thought\", \"legal_moves\", \"ascii_board\", \n",
    "                              \"unplayable_boards\"]\n",
    "    \n",
    "    columns_to_remove = [\n",
    "        col for col in raw_train_dataset.column_names \n",
    "        if col not in columns_needed_for_map and col != \"messages\"\n",
    "    ]\n",
    "    \n",
    "    train_dataset = raw_train_dataset.map(\n",
    "        format_data_for_training,\n",
    "        remove_columns = columns_to_remove,\n",
    "        batched=True\n",
    "    ).filter(lambda x: len(x['messages']) > 0)\n",
    "\n",
    "    eval_dataset = raw_eval_dataset.map(\n",
    "        format_data_for_training,\n",
    "        remove_columns = columns_to_remove,\n",
    "        batched=True\n",
    "    ).filter(lambda x: len(x['messages']) > 0)\n",
    "\n",
    "    print(\"VRAM Cleanup: Forcing garbage collection before training.\")\n",
    "    del raw_train_dataset, raw_eval_dataset \n",
    "    gc.collect() # Trigger Python garbage collection\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache() # Clear PyTorch's VRAM cache\n",
    "    print(\"Cleanup complete. Starting Trainer initialization.\")\n",
    "\n",
    "    print(\"Setting up training arguments...\")\n",
    "    training_arguments = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        per_device_eval_batch_size = 1,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 10,\n",
    "        learning_rate = 5e-5,\n",
    "        fp16 = False,\n",
    "        bf16 = True,\n",
    "        output_dir = OUTPUT_DIR_V3,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        seed = 42,\n",
    "        eval_strategy = \"epoch\",\n",
    "        eval_steps = 50,\n",
    "        logging_steps = 5,\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        save_strategy = \"epoch\",\n",
    "        save_steps = 100,\n",
    "        report_to = \"none\",\n",
    "        remove_unused_columns = False,\n",
    "        weight_decay = 0.01,\n",
    "    )\n",
    "\n",
    "    print(\"Initializing Trainer and starting fine-tuning...\")\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        args = training_arguments,\n",
    "        data_collator = MultimodalDataCollator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "    trainer.model.save_pretrained(OUTPUT_DIR_V3)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054536f1-f6e0-44ab-b4ff-6df9f9200abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model: unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\n",
      "==((====))==  Unsloth 2025.11.6: Fast Qwen3_Vl patching. Transformers: 4.57.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB MIG 4g.20gb. Num GPUs = 1. Max memory: 19.625 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ebb13362e3460db81e502ab9d89db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer loaded successfully\n",
      "Applying PEFT (QLoRA) layer...\n",
      "Loading training dataset...\n",
      "Dataset loaded with 800 examples.\n",
      "Loading evaluation dataset...\n",
      "Dataset loaded with 101 examples.\n",
      "Formatting datasets for multimodal training...\n",
      "VRAM Cleanup: Forcing garbage collection before training.\n",
      "Cleanup complete. Starting Trainer initialization.\n",
      "Setting up training arguments...\n",
      "Initializing Trainer and starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13978/4242294912.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer._unsloth___init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 800 | Num Epochs = 10 | Total steps = 1,000\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 30,670,848 of 8,797,794,544 (0.35% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 6:07:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.101200</td>\n",
       "      <td>0.959011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.876762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.845500</td>\n",
       "      <td>0.838616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.805900</td>\n",
       "      <td>0.818492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.737600</td>\n",
       "      <td>0.802874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.709900</td>\n",
       "      <td>0.789876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.735700</td>\n",
       "      <td>0.789669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.578200</td>\n",
       "      <td>0.794076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>0.794031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.645200</td>\n",
       "      <td>0.799393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Error: CUDA not detected!\")\n",
    "    else:\n",
    "        run_fine_tuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
