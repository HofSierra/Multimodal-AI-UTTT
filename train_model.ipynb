{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0929ac21-cbe3-47be-8e91-e985b4b44408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install unsloth transformers accelerate datasets bitsandbytes pandas pillow packaging ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a2449e-620a-41fd-a376-e6cc9ca3f40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, Trainer, AutoProcessor\n",
    "from datasets import Dataset, load_from_disk\n",
    "from PIL import Image as PILImage\n",
    "from PIL import ImageDraw, ImageFilter, ImageEnhance\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6943f1f2-17c1-469c-b944-9bbf949cbc37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "tokenizer = None\n",
    "processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e03f2e6-36f0-4c63-a78a-b48ebfe89d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vlm_model():\n",
    "    global model, tokenizer, processor\n",
    "    print(f\"Loading Model: {MODEL_NAME}\")\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = MODEL_NAME,\n",
    "        max_seq_length = MAX_SEQ_LENGTH,\n",
    "        dtype = DTYPE,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token_id = processor.tokenizer.pad_token_id \n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    print(\"Model and Tokenizer loaded successfully\")\n",
    "    return model, tokenizer, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976f6f1c-9380-45f3-bd71-e51d59af4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_spotlight_active_board(image, active_row, active_col):\n",
    "    if active_row < 0 or active_col < 0 or active_row > 2 or active_col > 2:\n",
    "        return image\n",
    "\n",
    "    width, height = image.size\n",
    "    \n",
    "    converter = ImageEnhance.Color(image)\n",
    "    desaturated = converter.enhance(0.5)\n",
    "    \n",
    "    converter = ImageEnhance.Brightness(desaturated)\n",
    "    darkened = converter.enhance(0.4)\n",
    "\n",
    "    board_size_w = width // 3\n",
    "    board_size_h = height // 3\n",
    "    \n",
    "    left = active_col * board_size_w\n",
    "    top = active_row * board_size_h\n",
    "    right = (active_col + 1) * board_size_w\n",
    "    bottom = (active_row + 1) * board_size_h\n",
    "\n",
    "    active_patch = image.crop((left, top, right, bottom))\n",
    "\n",
    "    darkened.paste(active_patch, (left, top))\n",
    "        \n",
    "    return darkened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d855ff-e96e-4027-aa98-6c3dfaf57381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultimodalDataCollator(data):\n",
    "    \"\"\"\n",
    "    Data collator responsible for batching, tokenization, image processing, \n",
    "    and applying label masking for multimodal training.\n",
    "\n",
    "    1. Extracts images and conversation messages.\n",
    "    2. Converts the conversation messages (System + User + Assistant) into a \n",
    "       single text prompt string using the tokenizer's chat template.\n",
    "    3. Processes images and tokenizes the full text prompt into a batch using \n",
    "       the VLM's multimodal processor.\n",
    "    4. CRITICALLY: Applies label masking (setting loss to -100) to all tokens \n",
    "       belonging to the System and User turns, ensuring the model only learns \n",
    "       to predict the Assistant's (target JSON) response.\n",
    "    \"\"\"\n",
    "    global model, processor, tokenizer\n",
    "    if model is None or processor is None or tokenizer is None:\n",
    "        raise ValueError(\"Model, processor, tokenizer must be loaded globally before using this.\")\n",
    "\n",
    "    # 1. Extract Images and Messages\n",
    "    images = [item[\"image\"].convert(\"RGB\") for item in data]\n",
    "    messages = [item[\"messages\"] for item in data]\n",
    "\n",
    "    # 2. Generate the raw prompt strings for the FULL conversation\n",
    "    text_prompts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for conversation in messages\n",
    "    ]\n",
    "\n",
    "    # 3. Process Images and Tokenize Text (Full Batch)\n",
    "    try: \n",
    "        # The processor handles both image preprocessing (to image_pixel_values) \n",
    "        # and full text tokenization for the VLM, resulting in a padded batch.\n",
    "        inputs = processor(\n",
    "            images = images,\n",
    "            text = text_prompts,\n",
    "            return_tensors = \"pt\",\n",
    "            padding = True,\n",
    "            max_length = processor.tokenizer.model_max_length,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to create batch. Actual error: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 4. Manual Label Masking: Clone input_ids to create the initial labels tensor\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    # 5. Determine the masking cutoff point for each example\n",
    "    for i, conversation in enumerate(messages):\n",
    "        # We need the tokenized length of the [system, user] part to know where to stop masking\n",
    "        system_user_conversation = conversation[:2] \n",
    "        \n",
    "        # A. Generate the System + User string (text only)\n",
    "        # we must use the full multimodal processor on the System + User part. \n",
    "        # The previous attempt failed because the text-only tokenizer does not \n",
    "        # account for the image patch tokens inserted by the VLM processor.\n",
    "        \n",
    "        # 1. Generate the System + User string (includes <image> placeholder)\n",
    "        system_user_text = tokenizer.apply_chat_template(\n",
    "            system_user_conversation,\n",
    "            tokenize=False,\n",
    "            # This is critical to include the final <|assistant|> turn token\n",
    "            add_generation_prompt=True \n",
    "        )\n",
    "\n",
    "        # 2. Use the FULL multimodal processor on this single example to get the length.\n",
    "        # This correctly tokenizes the text and accounts for the image tokens.\n",
    "        try:\n",
    "            system_user_tokens = processor(\n",
    "                images = [images[i]], # Pass the single image from the batch\n",
    "                text = system_user_text,\n",
    "                return_tensors = \"pt\",\n",
    "                padding = False, # Do not pad single example\n",
    "            ).input_ids.squeeze()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing single prompt for masking length: {e}\")\n",
    "            system_user_tokens = torch.tensor([-1])\n",
    "\n",
    "\n",
    "        # Calculate the length for masking\n",
    "        if system_user_tokens.dim() == 0 or system_user_tokens[0] == -1:\n",
    "             # Handle error case by skipping (loss will be computed for full sequence, which is wrong)\n",
    "             print(f\"Warning: Masking calculation failed for sample {i}. Using safe, non-masked sequence.\")\n",
    "             cutoff_length = 0 # No mask\n",
    "        else:\n",
    "            # The number of tokens corresponding to the System + User part + <|assistant|> token\n",
    "            cutoff_length = system_user_tokens.shape[0]\n",
    "\n",
    "        # Mask out the System and User tokens in the padded batch by setting them to -100\n",
    "        # The loss function ignores tokens with label -100.\n",
    "        labels[i, :cutoff_length] = -100\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2dd9f71-6fc6-40ff-8dd5-65a947ed2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_training(examples):\n",
    "    messages_list = []\n",
    "    batch_size = len(examples.get(\"image\", []))\n",
    "\n",
    "    if batch_size == 0:\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    coordinate_definition = (\n",
    "        \"**COORDINATE SYSTEM DEFINITION:**\\n\"\n",
    "        \"The board uses a 0-indexed system (0, 1, 2) for both rows and columns.\\n\"\n",
    "        \"Row 0 is the TOP row. Column 0 is the LEFT column.\\n\"\n",
    "        \"Your output must STRICTLY use the integers 0, 1, or 2 for all coordinates.\\n\"\n",
    "        \"Example: The top-left corner is (0, 0).\"\n",
    "    )\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        image = examples.get(\"image\", [None]*batch_size)[i]\n",
    "        \n",
    "        if not isinstance(image, PILImage.Image):\n",
    "            messages_list.append([])\n",
    "            continue\n",
    "\n",
    "        allowed_squares = examples[\"allowed_squares\"][i]\n",
    "        unplayable_boards = examples[\"unplayable_boards\"][i]\n",
    "        best_move_dict = examples[\"best_move\"][i]\n",
    "        player_turn = examples[\"player\"][i]\n",
    "        cot_text = examples[\"chain_of_thought\"][i]\n",
    "        ascii_board = examples[\"ascii_board\"][i]\n",
    "            \n",
    "        if isinstance(allowed_squares, (list, tuple)) and len(allowed_squares) == 2:\n",
    "            ACTIVE_GLOBAL_ROW = allowed_squares[0]\n",
    "            ACTIVE_GLOBAL_COL = allowed_squares[1]\n",
    "        else:\n",
    "            ACTIVE_GLOBAL_ROW = -1\n",
    "            ACTIVE_GLOBAL_COL = -1\n",
    "\n",
    "        unplayable_list_str = json.dumps(unplayable_boards)    \n",
    "        best_move_json = json.dumps(best_move_dict)\n",
    "\n",
    "        transformed_image = visual_spotlight_active_board(image, ACTIVE_GLOBAL_ROW, ACTIVE_GLOBAL_COL)\n",
    "        examples[\"image\"][i] = transformed_image\n",
    "\n",
    "        system_content = (\n",
    "            f\"You are an expert Ultimate Tic-Tac-Toe player. \"\n",
    "            f\"Your goal is to identify the optimal, legal move based on the provided image and context. \"\n",
    "            f\"{coordinate_definition}\\n\\n\"\n",
    "            f\"The final output must be **ONLY** a raw JSON object containing the chosen move.\"\n",
    "        )\n",
    "\n",
    "        # --- User Prompt (provides all game context) ---\n",
    "        user_prompt = (\n",
    "            f\"Player: {player_turn} (X=Player 1, O=Player 2)\\n\"\n",
    "            f\"Analyze the board state in the image and determine the optimal move.\\n\\n\"\n",
    "            \n",
    "            f\"--- CRITICAL MOVE RESTRICTION ---\\n\"\n",
    "            f\"The global board highlighted in **BRIGHT GREEN** (and visually emphasized) is the current active board constraint. You MUST select a local cell within this board unless it is Free Play (where no board is emphasized).\\n\"\n",
    "            f\"**Unplayable Boards:** The following Global Boards are already WON or TIED and cannot be played: {unplayable_list_str}\\n\\n\"\n",
    "            \n",
    "            f\"--- ASCII VISUALIZATION ---\\n\"\n",
    "            f\"Use this labeled diagram to cross-reference the image coordinates (0, 1, 2) with the piece locations and board status:\\n\"\n",
    "            f\"{ascii_board}\\n\\n\"\n",
    "\n",
    "            f\"CRITICAL RULE: The target local cell (local_row, local_col) MUST be **EMPTY** on the active global board.\\n\"\n",
    "            f\"CRITICAL RULE: All output coordinates (global_row, global_col, local_row, local_col) MUST be **0, 1 or 2**.\"\n",
    "        )\n",
    "\n",
    "        assistant_content = best_move_json\n",
    "            \n",
    "        messages_for_current_example = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_content}]},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": user_prompt},\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_content}]}\n",
    "        ]\n",
    "        messages_list.append(messages_for_current_example)\n",
    "\n",
    "    examples[\"messages\"] = messages_list\n",
    "    return examples\n",
    "\n",
    "def format_data_for_eval(examples):\n",
    "    messages_list = []\n",
    "    batch_size = len(examples.get(\"image\", []))\n",
    "\n",
    "    if batch_size == 0:\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    coordinate_definition = (\n",
    "        \"**COORDINATE SYSTEM DEFINITION:**\\n\"\n",
    "        \"The board uses a 0-indexed system (0, 1, 2) for both rows and columns.\\n\"\n",
    "        \"Row 0 is the TOP row. Column 0 is the LEFT column.\\n\"\n",
    "        \"Your output must STRICTLY use the integers 0, 1, or 2 for all coordinates.\\n\"\n",
    "        \"Example: The top-left corner is (0, 0).\"\n",
    "    )\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        image = examples.get(\"image\", [None]*batch_size)[i]\n",
    "        \n",
    "        if not isinstance(image, PILImage.Image):\n",
    "            messages_list.append([])\n",
    "            continue\n",
    "\n",
    "        allowed_squares = examples[\"allowed_squares\"][i]\n",
    "        unplayable_boards = examples[\"unplayable_boards\"][i]\n",
    "        best_move_dict = examples[\"best_move\"][i]\n",
    "        player_turn = examples[\"player\"][i]\n",
    "        cot_text = examples[\"chain_of_thought\"][i]\n",
    "        ascii_board = examples[\"ascii_board\"][i]\n",
    "\n",
    "        unplayable_list_str = json.dumps(unplayable_boards)    \n",
    "        best_move_json = json.dumps(best_move_dict)\n",
    "\n",
    "        system_content = (\n",
    "            f\"You are an expert Ultimate Tic-Tac-Toe player. \"\n",
    "            f\"Your goal is to identify the optimal, legal move based on the provided image and context. \"\n",
    "            f\"{coordinate_definition}\\n\\n\"\n",
    "            f\"The final output must be **ONLY** a raw JSON object containing the chosen move.\"\n",
    "        )\n",
    "\n",
    "        user_prompt = (\n",
    "            f\"Player: {player_turn} (X=Player 1, O=Player 2)\\n\"\n",
    "            f\"Analyze the board state in the image and determine the optimal move.\\n\\n\"\n",
    "            \n",
    "            f\"--- CRITICAL MOVE RESTRICTION ---\\n\"\n",
    "            f\"The global board highlighted in **BRIGHT GREEN** is the current active board constraint. You MUST select a local cell within this board unless it is Free Play (where no board is highlighted).\\n\"\n",
    "            f\"**Unplayable Boards:** The following Global Boards are already WON or TIED and cannot be played: {unplayable_list_str}\\n\\n\"\n",
    "            \n",
    "            f\"--- ASCII VISUALIZATION ---\\n\"\n",
    "            f\"Use this labeled diagram to cross-reference the image coordinates (0, 1, 2) with the piece locations and board status:\\n\"\n",
    "            f\"{ascii_board}\\n\\n\"\n",
    "\n",
    "            f\"CRITICAL RULE: The target local cell (local_row, local_col) MUST be **EMPTY** on the active global board.\\n\"\n",
    "            f\"CRITICAL RULE: All output coordinates (global_row, global_col, local_row, local_col) MUST be **0, 1 or 2**.\"\n",
    "        )\n",
    "\n",
    "        assistant_content = best_move_json\n",
    "            \n",
    "        messages_for_current_example = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_content}]},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": user_prompt},\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_content}]}\n",
    "        ]\n",
    "        messages_list.append(messages_for_current_example)\n",
    "\n",
    "    examples[\"messages\"] = messages_list\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6835286b-f793-48a9-9365-eb40a8178f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_remove(raw_dataset, columns_needed_for_map):\n",
    "    return [\n",
    "        col for col in raw_dataset.column_names \n",
    "        if col not in columns_needed_for_map and col != \"messages\"\n",
    "    ]\n",
    "\n",
    "def run_fine_tuning():\n",
    "    model, tokenizer, processor = load_vlm_model()\n",
    "\n",
    "    print(\"Applying PEFT (QLoRA) layer...\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = 32,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_alpha = 32,\n",
    "        lora_dropout = 0,\n",
    "        bias = \"none\",\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        random_state = 42,\n",
    "        embedding_layer_names = [\"vision_tower.image_projection\"]\n",
    "    )\n",
    "\n",
    "    print(\"Loading training dataset...\")\n",
    "    try: \n",
    "        raw_train_dataset = Dataset.from_parquet(DATASET_TRAIN_PATH)\n",
    "        print(f\"Dataset loaded with {len(raw_train_dataset)} examples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Loading evaluation dataset...\")\n",
    "    try: \n",
    "        raw_eval_dataset = Dataset.from_parquet(DATASET_EVALUATION_PATH)\n",
    "        print(f\"Dataset loaded with {len(raw_eval_dataset)} examples.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Formatting datasets for multimodal training...\")\n",
    "\n",
    "    columns_needed_for_map = [\"image\", \"player\", \"allowed_squares\", \"best_move\",\n",
    "                              \"chain_of_thought\", \"legal_moves\", \"ascii_board\", \n",
    "                              \"unplayable_boards\"]\n",
    "    \n",
    "    columns_to_remove_train = get_columns_to_remove(raw_train_dataset, columns_needed_for_map)\n",
    "    columns_to_remove_eval = get_columns_to_remove(raw_eval_dataset, columns_needed_for_map)\n",
    "    \n",
    "    train_dataset = raw_train_dataset.map(\n",
    "        format_data_for_training,\n",
    "        remove_columns = columns_to_remove_train,\n",
    "        batched=True\n",
    "    ).filter(lambda x: len(x['messages']) > 0)\n",
    "\n",
    "    eval_dataset = raw_eval_dataset.map(\n",
    "        format_data_for_eval,\n",
    "        remove_columns = columns_to_remove_eval,\n",
    "        batched=True\n",
    "    ).filter(lambda x: len(x['messages']) > 0)\n",
    "\n",
    "    print(\"Setting up training arguments...\")\n",
    "    training_arguments = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        per_device_eval_batch_size = 1,\n",
    "        gradient_accumulation_steps = 16,\n",
    "        warmup_steps = 40,\n",
    "        num_train_epochs = 8,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = False,\n",
    "        bf16 = True,\n",
    "        output_dir = OUTPUT_DIR_V4,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        seed = 42,\n",
    "        eval_strategy = \"epoch\",\n",
    "        eval_steps = 50,\n",
    "        logging_steps = 5,\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        save_strategy = \"epoch\",\n",
    "        save_steps = 200,\n",
    "        report_to = \"none\",\n",
    "        remove_unused_columns = False,\n",
    "        weight_decay = 0.01,\n",
    "    )\n",
    "\n",
    "    print(\"Initializing Trainer and starting fine-tuning...\")\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        args = training_arguments,\n",
    "        data_collator = MultimodalDataCollator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "    trainer.model.save_pretrained(OUTPUT_DIR_V4)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR_V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "054536f1-f6e0-44ab-b4ff-6df9f9200abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model: unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\n",
      "==((====))==  Unsloth 2025.11.6: Fast Qwen3_Vl patching. Transformers: 4.57.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB MIG 4g.20gb. Num GPUs = 1. Max memory: 19.625 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb643f96f0a4b26abede0e8dd35185f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer loaded successfully\n",
      "Applying PEFT (QLoRA) layer...\n",
      "Loading training dataset...\n",
      "Dataset loaded with 3203 examples.\n",
      "Loading evaluation dataset...\n",
      "Dataset loaded with 401 examples.\n",
      "Formatting datasets for multimodal training...\n",
      "Setting up training arguments...\n",
      "Initializing Trainer and starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11116/4233999942.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer._unsloth___init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 3,203 | Num Epochs = 8 | Total steps = 1,608\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 30,670,848 of 8,797,794,544 (0.35% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1608' max='1608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1608/1608 8:41:37, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.078109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.069689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.065381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.063964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.063729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.065534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.066580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.068977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Error: CUDA not detected!\")\n",
    "    else:\n",
    "        run_fine_tuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
