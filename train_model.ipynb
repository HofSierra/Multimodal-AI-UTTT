{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0929ac21-cbe3-47be-8e91-e985b4b44408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "#!pip install unsloth transformers accelerate datasets bitsandbytes pandas pillow packaging ninja"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a2449e-620a-41fd-a376-e6cc9ca3f40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json, random\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, Trainer, AutoProcessor\n",
    "from datasets import Dataset\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6943f1f2-17c1-469c-b944-9bbf949cbc37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "tokenizer = None\n",
    "processor = None"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the model",
   "id": "1b013aa747b40bb3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e03f2e6-36f0-4c63-a78a-b48ebfe89d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vlm_model():\n",
    "    global model, tokenizer, processor\n",
    "    print(f\"Loading Model: {MODEL_NAME}\")\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = MODEL_NAME,\n",
    "        max_seq_length = 4096,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    print(\"Model and Tokenizer loaded successfully\")\n",
    "    return model, tokenizer, processor"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Collator",
   "id": "ce50d89a2e08f9a7"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d855ff-e96e-4027-aa98-6c3dfaf57381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultimodalDataCollator(batch):\n",
    "    global model, processor, tokenizer\n",
    "\n",
    "    images = [item[\"image\"].convert(\"RGB\") for item in batch]\n",
    "    messages = [item[\"messages\"] for item in batch]\n",
    "\n",
    "    texts = [\n",
    "        processor.apply_chat_template(\n",
    "            msg,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for msg in messages\n",
    "    ]\n",
    "\n",
    "    inputs = processor(\n",
    "        images=images,\n",
    "        text=texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    labels = input_ids.clone()\n",
    "    token_handler = processor.tokenizer\n",
    "    \n",
    "    im_start_token_id = token_handler.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "    \n",
    "    if im_start_token_id is None:\n",
    "        im_start_token_id = 151644 \n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        start_indices = (input_ids[i] == im_start_token_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(start_indices) == 0:\n",
    "            labels[i, :] = -100\n",
    "            continue\n",
    "            \n",
    "        last_start_idx = start_indices[-1].item()\n",
    "        mask_end_idx = last_start_idx + 3\n",
    "        mask_end_idx = min(mask_end_idx, len(labels[i]))\n",
    "        \n",
    "        labels[i, :mask_end_idx] = -100\n",
    "        if token_handler.pad_token_id is not None:\n",
    "             labels[i][input_ids[i] == token_handler.pad_token_id] = -100\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Formatting the data in batches for the training",
   "id": "a30a7fd5d3db6a54"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2dd9f71-6fc6-40ff-8dd5-65a947ed2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(examples):\n",
    "    messages_list = []\n",
    "    batch_size = len(next(iter(examples.values())))\n",
    "\n",
    "    coordinate_definition = (\n",
    "        \"**STRICT PROTOCOL:**\\n\"\n",
    "        \"1. FORMAT: All responses must be valid JSON snippets wrapped in JSON_START/JSON_END anchors.\\n\"\n",
    "        \"2. COORDINATES: Use 0-indexed integers (0, 1, 2) for all Row/Col values.\\n\"\n",
    "        \"3. VISUAL ANCHOR: The allowed square (active subgrid) is highlighted in GREEN. You must play there.\"\n",
    "    )\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        raw_allowed = examples[\"allowed_squares\"][i]\n",
    "        \n",
    "        ag_r, ag_c = None, None\n",
    "        if raw_allowed and len(raw_allowed) >= 2:\n",
    "            ag_r, ag_c = raw_allowed[0], raw_allowed[1]\n",
    "            gt_active_json = {\"global_row\": ag_r, \"global_col\": ag_c}\n",
    "        else:\n",
    "            gt_active_json = None\n",
    "        \n",
    "        image = examples[\"image\"][i]\n",
    "        rot_angle = examples[\"rotation_angle\"][i]\n",
    "        player_val = examples[\"player\"][i]\n",
    "        player_char = \"X\" if player_val == 1 else \"O\"\n",
    "        best_move = examples[\"best_move\"][i]\n",
    "        legal_moves = examples[\"legal_moves\"][i]\n",
    "        full_state = examples[\"global_state\"][i]\n",
    "        original_cot = examples[\"chain_of_thought\"][i]\n",
    "        unplayable = examples[\"unplayable_boards\"][i]\n",
    "        \n",
    "        system_content = f\"You are an Ultimate Tic-Tac-Toe Visual Engine. {coordinate_definition}\"\n",
    "        \n",
    "        if rot_angle == 0:\n",
    "            task_type = random.choices(\n",
    "                [\"ALLOWED_SQUARE\", \"MOVE\", \"STATE\", \"LEGALITY\"], \n",
    "                weights=[10, 50, 20, 20],\n",
    "                k=1\n",
    "            )[0]\n",
    "        else:\n",
    "            task_type = random.choices(\n",
    "                [\"ALLOWED_SQUARE\", \"STATE\", \"LEGALITY\"], \n",
    "                weights=[20, 40, 40],\n",
    "                k=1\n",
    "            )[0]\n",
    "        \n",
    "        assistant_content = \"\"\n",
    "\n",
    "        # --- 1. ALLOWED_SQUARE ---\n",
    "        if task_type == \"ALLOWED_SQUARE\":\n",
    "            user_prompt = \"Identify the allowed square (active global subgrid) based on the green highlight. If none is highlighted, report null.\"\n",
    "            assistant_content = f\"JSON_START\\n{{\\\"allowed_square\\\": {json.dumps(gt_active_json)}}}\\nJSON_END\"\n",
    "\n",
    "        # --- 2. MOVE ---\n",
    "        elif task_type == \"MOVE\":\n",
    "            user_prompt = f\"Visually analyze the board. It is Player {player_char}'s turn. Identify the allowed square (green highlight) and then select the best move.\"\n",
    "            \n",
    "            final_move_json = {\n",
    "                \"global_row\": best_move[\"global_row\"], \"global_col\": best_move[\"global_col\"], \n",
    "                \"local_row\": best_move[\"local_row\"], \"local_col\": best_move[\"local_col\"]\n",
    "            }\n",
    "\n",
    "            clean_cot = original_cot\n",
    "            # removing the final move in cot to avoid repetition and token wastage\n",
    "            if \"FINAL MOVE:\" in original_cot:\n",
    "                clean_cot = original_cot.split(\"FINAL MOVE:\")[0].strip()\n",
    "\n",
    "            if ag_r is not None:\n",
    "                # force the model to also analyze the state in move\n",
    "                scan_text = f\"Active Global Board is ({ag_r}, {ag_c}). Visual Scan:\\n\"\n",
    "                empty_spots = []\n",
    "                \n",
    "                for r in range(3):\n",
    "                    row_desc = []\n",
    "                    for c in range(3):\n",
    "                        occ = next((p['player'] for p in full_state \n",
    "                                    if p['global_row'] == ag_r and p['global_col'] == ag_c \n",
    "                                    and p['local_row'] == r and p['local_col'] == c), 0)\n",
    "                        \n",
    "                        symbol = \"X\" if occ == 1 else \"O\" if occ == 2 else \".\"\n",
    "                        row_desc.append(symbol)\n",
    "                        if occ == 0:\n",
    "                            empty_spots.append(f\"({r},{c})\")\n",
    "                    scan_text += f\"Row {r}: {row_desc}\\n\"\n",
    "                \n",
    "                scan_text += f\"Available Squares: {', '.join(empty_spots)}.\\n\"\n",
    "                scan_text += f\"Strategy: {clean_cot}\"\n",
    "            else:\n",
    "                scan_text = f\"No active constraint. Free move. {clean_cot}\"\n",
    "\n",
    "            assistant_content = (\n",
    "                \"JSON_START\\n\"\n",
    "                f'{{\"allowed_square\": {json.dumps(gt_active_json)}, '\n",
    "                f'\"thinking\": {json.dumps(scan_text)}, ' \n",
    "                f'\"best_move\": {json.dumps(final_move_json)}}}'\n",
    "                \"\\nJSON_END\"\n",
    "            )\n",
    "\n",
    "        # --- 3. STATE ---\n",
    "        elif task_type == \"STATE\":\n",
    "            target_r, target_c = None, None\n",
    "\n",
    "            # improve recognition of the smaller grids in the allowed square\n",
    "            if raw_allowed and len(raw_allowed) >= 2 and random.random() < 0.5:\n",
    "                 target_r, target_c = raw_allowed[0], raw_allowed[1]\n",
    "\n",
    "            # otherwise make it look at populated subgrids rather than empty ones\n",
    "            if target_r is None:\n",
    "                occupied_globals = list(set((x['global_row'], x['global_col']) for x in full_state))\n",
    "                if occupied_globals:\n",
    "                    target_r, target_c = random.choice(occupied_globals)\n",
    "\n",
    "            # start of the game where occupied_boards might be empty\n",
    "            if target_r is None:\n",
    "                target_r, target_c = random.randint(0, 2), random.randint(0, 2)\n",
    "\n",
    "            user_prompt = f\"Examine Global Subgrid ({target_r}, {target_c}). Represent the 3x3 local grid state as a matrix of 'X', 'O', or '.' (Empty).\"\n",
    "            \n",
    "            matrix = []\n",
    "            for lr in range(3):\n",
    "                row_list = []\n",
    "                for lc in range(3):\n",
    "                    occ = next((p['player'] for p in full_state if p['global_row'] == target_r and p['global_col'] == target_c and p['local_row'] == lr and p['local_col'] == lc), 0)\n",
    "                    val = \"X\" if occ == 1 else \"O\" if occ == 2 else \".\"\n",
    "                    row_list.append(val)\n",
    "                matrix.append(row_list)\n",
    "\n",
    "            assistant_content = f\"JSON_START\\n{{\\\"target_global\\\": [{target_r}, {target_c}], \\\"grid_matrix\\\": {json.dumps(matrix)}}}\\nJSON_END\"\n",
    "\n",
    "        # --- 4. LEGALITY ---\n",
    "        elif task_type == \"LEGALITY\":\n",
    "            choice_roll = random.random()\n",
    "            move = None\n",
    "\n",
    "            # pick an unplayable board so that the model learns to not play in completed boards\n",
    "            if choice_roll < 0.40 and unplayable:\n",
    "                dead_board = random.choice(unplayable)\n",
    "                \n",
    "                db_r = dead_board[0] if isinstance(dead_board, list) else dead_board['global_row']\n",
    "                db_c = dead_board[1] if isinstance(dead_board, list) else dead_board['global_col']\n",
    "                \n",
    "                move = {\n",
    "                    \"global_row\": db_r, \"global_col\": db_c, \n",
    "                    \"local_row\": random.randint(0, 2), \"local_col\": random.randint(0, 2)\n",
    "                }\n",
    "\n",
    "            # otherwise some random grid\n",
    "            elif choice_roll < 0.80 and full_state:\n",
    "                p = random.choice(full_state)\n",
    "                move = {\n",
    "                    \"global_row\": p['global_row'], \"global_col\": p['global_col'], \n",
    "                    \"local_row\": p['local_row'], \"local_col\": p['local_col']\n",
    "                }\n",
    "\n",
    "            # otherwise some legal move\n",
    "            elif legal_moves:\n",
    "                move = random.choice(legal_moves)\n",
    "                \n",
    "            # start of a game as the other checks would fail\n",
    "            else:\n",
    "                move = {\n",
    "                    \"global_row\": random.randint(0, 2), \"global_col\": random.randint(0, 2), \n",
    "                    \"local_row\": random.randint(0, 2), \"local_col\": random.randint(0, 2)\n",
    "                }\n",
    "\n",
    "            is_legal = any(m['global_row'] == move['global_row'] and m['global_col'] == move['global_col'] and \n",
    "                           m['local_row'] == move['local_row'] and m['local_col'] == move['local_col'] for m in legal_moves)\n",
    "            \n",
    "            occ = next((p['player'] for p in full_state if p['global_row'] == move['global_row'] and \n",
    "                        p['global_col'] == move['global_col'] and p['local_row'] == move['local_row'] and \n",
    "                        p['local_col'] == move['local_col']), 0)\n",
    "            sq_state = \"X\" if occ == 1 else \"O\" if occ == 2 else \"Empty\"\n",
    "\n",
    "            is_dead_board = False\n",
    "            if unplayable:\n",
    "                for u in unplayable:\n",
    "                    u_r = u[0] if isinstance(u, list) else u.get('global_row')\n",
    "                    u_c = u[1] if isinstance(u, list) else u.get('global_col')\n",
    "                    if u_r == move['global_row'] and u_c == move['global_col']:\n",
    "                        is_dead_board = True\n",
    "                        break\n",
    "\n",
    "            if is_legal:\n",
    "                reason = \"Square is Empty, board is playable, and within active constraint.\"\n",
    "            else:\n",
    "                if is_dead_board:\n",
    "                    reason = f\"Global Board ({move['global_row']},{move['global_col']}) is already completed.\"\n",
    "                \n",
    "                elif sq_state != \"Empty\":\n",
    "                    reason = f\"Square is occupied by {sq_state}.\"\n",
    "                \n",
    "                else:\n",
    "                    reason = \"Violates allowed square constraint.\"\n",
    "\n",
    "            user_prompt = (\n",
    "                f\"Is it legal for Player {player_char} to play at Global({move['global_row']},{move['global_col']}), Local({move['local_row']},{move['local_col']})? \"\n",
    "                \"Step 1: Inspect the square state. Step 2: Check allowed square constraint. Step 3: Verdict.\"\n",
    "            )\n",
    "\n",
    "            assistant_content = (\n",
    "                \"JSON_START\\n\"\n",
    "                f'{{\"step_1_square_state\": \"{sq_state}\", '\n",
    "                f'\"step_2_allowed_square\": {json.dumps(gt_active_json)}, ' \n",
    "                f'\"is_legal\": {json.dumps(is_legal)}, '\n",
    "                f'\"reason\": {json.dumps(reason)}}}'\n",
    "                \"\\nJSON_END\"\n",
    "            )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_content}]},\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}, {\"type\": \"image\"}]},\n",
    "            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_content}]}\n",
    "        ]\n",
    "        messages_list.append(messages)\n",
    "\n",
    "    examples[\"messages\"] = messages_list\n",
    "    return examples"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training & Finetuning",
   "id": "7d38d4b996880b90"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6835286b-f793-48a9-9365-eb40a8178f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_remove(raw_dataset, columns_needed_for_map):\n",
    "    return [\n",
    "        col for col in raw_dataset.column_names \n",
    "        if col not in columns_needed_for_map and col != \"messages\"\n",
    "    ]\n",
    "\n",
    "def run_fine_tuning():\n",
    "    model, tokenizer, processor = load_vlm_model()\n",
    "\n",
    "    print(\"Applying PEFT (QLoRA) layer...\")\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=128,\n",
    "        target_modules=\"all-linear\",\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=42,\n",
    "        embedding_layer_names=[\"visual\"],\n",
    "    )\n",
    "\n",
    "    print(\"Loading training dataset...\")\n",
    "    raw_train_dataset = Dataset.from_parquet(DATASET_TRAIN_PATH)\n",
    "\n",
    "    print(\"Loading evaluation dataset...\")\n",
    "    raw_eval_dataset = Dataset.from_parquet(DATASET_EVAL_PATH)\n",
    "\n",
    "    columns_needed_for_map = [\"image\", \"player\", \"allowed_squares\", \"best_move\",\n",
    "                              \"chain_of_thought\", \"legal_moves\", \"ascii_board\", \n",
    "                              \"unplayable_boards\", \"global state\", \"rotation_angle\"]\n",
    "    \n",
    "    columns_to_remove_train = get_columns_to_remove(raw_train_dataset, columns_needed_for_map)\n",
    "    columns_to_remove_eval = get_columns_to_remove(raw_eval_dataset, columns_needed_for_map)\n",
    "    \n",
    "    train_dataset = raw_train_dataset.map(\n",
    "        format_data,\n",
    "        remove_columns = columns_to_remove_train,\n",
    "        batched=True\n",
    "    ).filter(lambda x: len(x['messages']) > 0)\n",
    "\n",
    "    eval_dataset = raw_eval_dataset.map(\n",
    "        format_data,\n",
    "        remove_columns = columns_to_remove_eval,\n",
    "        batched=True\n",
    "    ).filter(lambda x: len(x['messages']) > 0)\n",
    "\n",
    "    print(\"Setting up training arguments...\")\n",
    "    training_arguments = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        per_device_eval_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 4,\n",
    "        \n",
    "        learning_rate = 2e-4,\n",
    "        max_grad_norm = 0.3,\n",
    "        weight_decay = 0.05,\n",
    "        \n",
    "        fp16 = False,\n",
    "        bf16 = True,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        output_dir = OUTPUT_DIR_V2,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        seed = 42,\n",
    "        \n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 100,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 100,\n",
    "        logging_steps = 5,\n",
    "        \n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        save_total_limit = 5,\n",
    "        report_to = \"none\",\n",
    "        remove_unused_columns = False,\n",
    "        gradient_checkpointing = True, \n",
    "    )\n",
    "\n",
    "    print(\"Initializing Trainer and starting fine-tuning...\")\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        args = training_arguments,\n",
    "        data_collator = MultimodalDataCollator,\n",
    "    )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "    \n",
    "    print(\"\\nTraining complete.\")\n",
    "    trainer.model.save_pretrained(OUTPUT_DIR_V2)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054536f1-f6e0-44ab-b4ff-6df9f9200abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model: unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\n",
      "==((====))==  Unsloth 2026.1.4: Fast Qwen3_Vl patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA A100 80GB PCIe MIG 1g.20gb. Num GPUs = 1. Max memory: 19.5 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec80a2ca0734ae0a05595a99b35f302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer loaded successfully\n",
      "Applying PEFT (QLoRA) layer...\n",
      "Loading training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da6280fd96d44318d2dfc3e7eb14c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 3203 examples.\n",
      "Loading evaluation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be04ed2ad5714463a7db70fafaba6f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 401 examples.\n",
      "Formatting datasets for multimodal training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe8bfe4db2e40f7b328195014f4bc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7492dd719f5f45ce9e14cf26a472a18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383cb52f481443ec8bfd9fee69108932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c09882f3474fb29cbdcf0a38b57443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training arguments...\n",
      "Initializing Trainer and starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14253/3788174691.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer._unsloth___init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 3,203 | Num Epochs = 4 | Total steps = 804\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 410,775,552 of 9,177,899,248 (4.48% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='804' max='804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [804/804 7:43:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.082992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.082210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.156200</td>\n",
       "      <td>0.074895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.076684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.089200</td>\n",
       "      <td>0.076509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.090780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.091683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_fine_tuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
